{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a655a527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Idme</th>\n",
       "      <th>Q9_Loneliness_Meaning_Qual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>203</td>\n",
       "      <td>-99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>237</td>\n",
       "      <td>Makes me feel empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>545</td>\n",
       "      <td>Not speaking to anyone for days. Being out wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>565</td>\n",
       "      <td>I am a very loving generous person but due to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>658</td>\n",
       "      <td>Unlikeable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Idme                         Q9_Loneliness_Meaning_Qual\n",
       "0   203                                                -99\n",
       "1   237                                Makes me feel empty\n",
       "2   545  Not speaking to anyone for days. Being out wit...\n",
       "3   565  I am a very loving generous person but due to ...\n",
       "4   658                                         Unlikeable"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "## Imports (code & data)\n",
    "\n",
    "import pandas as pd\n",
    "import yake\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "raw_data = pd.read_csv('C:\\\\Users\\BRIGHT\\\\Desktop\\\\caproject.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d636d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming column\n",
    "\n",
    "raw_data = raw_data.rename(columns = {'Q9_Loneliness_Meaning_Qual': 'response'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48000f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = raw_data[raw_data['response'] != '-99']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b4baa7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39444, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aba738de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-f0c347be4c19>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['response'] = clean_data['response'].str.lower()\n",
      "<ipython-input-5-f0c347be4c19>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['response'] = clean_data['response'].str.replace('loneliness', '')\n",
      "<ipython-input-5-f0c347be4c19>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['response'] = clean_data['response'].str.replace('/', ' ')\n",
      "<ipython-input-5-f0c347be4c19>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['response'] = clean_data['response'].str.replace('\\n', ' ')\n",
      "<ipython-input-5-f0c347be4c19>:5: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  clean_data['response'] = clean_data['response'].str.replace('.', '')\n",
      "<ipython-input-5-f0c347be4c19>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['response'] = clean_data['response'].str.replace('.', '')\n",
      "<ipython-input-5-f0c347be4c19>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['response'] = clean_data['response'].str.replace(',', '')\n"
     ]
    }
   ],
   "source": [
    "clean_data['response'] = clean_data['response'].str.lower()\n",
    "clean_data['response'] = clean_data['response'].str.replace('loneliness', '')\n",
    "clean_data['response'] = clean_data['response'].str.replace('/', ' ')\n",
    "clean_data['response'] = clean_data['response'].str.replace('\\n', ' ')\n",
    "clean_data['response'] = clean_data['response'].str.replace('.', '')\n",
    "clean_data['response'] = clean_data['response'].str.replace(',', '')\n",
    "                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4fc853e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-72-35bdf9cf5b85>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-72-35bdf9cf5b85>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    clean_data['response'] = clean_data['response'].wnl.lemmatize(word) for word in clean_data['response']\u001b[0m\n\u001b[1;37m                                                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from nltk import stem\n",
    "wnl = stem.WordNetLemmatizer()\n",
    "porter = stem.porter.PorterStemmer()\n",
    "\n",
    "clean_data['response'] = clean_data['response'].wnl.lemmatize(word) for word in clean_data['response']]\n",
    "\n",
    "[porter.stem(word) for word in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5acc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['response'] = clean_data['response'].str.replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f3a5367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "documents =clean_data['response'].values.astype(\"U\")\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = ('english'))\n",
    "features = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3e6abed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = features[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dd7845a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 18677 into shape (8, 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-c8425d3c3f0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# If the shape already matches, don't bother doing an actual reshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;31m# Otherwise, the default is to convert to COO and use its reshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_reshape_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\sputils.py\u001b[0m in \u001b[0;36mcheck_shape\u001b[1;34m(args, current_shape)\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mnew_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnew_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mcurrent_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m                 raise ValueError('cannot reshape array of size {} into shape {}'\n\u001b[0m\u001b[0;32m    308\u001b[0m                                  .format(current_size, new_shape))\n\u001b[0;32m    309\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnegative_indexes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 18677 into shape (8, 8)"
     ]
    }
   ],
   "source": [
    "img = data[0].reshape(8,8)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5b7f8801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7657f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, perplexity=20)\n",
    "data_2d = tsne.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d076634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0114703",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2eaa58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= DBSCAN(eps = 0.8, min_samples = 25).fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f5887cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-973a3818abf2>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['cluster'] = model.labels_\n"
     ]
    }
   ],
   "source": [
    "clean_data['cluster'] = model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ad85484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False,  True,  True])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Firts, create an array of booleans using the labels from db.\n",
    "core_samples_mask = np.zeros_like(clean_data['cluster'], dtype=bool)\n",
    "core_samples_mask[model.core_sample_indices_] = True\n",
    "core_samples_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5eba004a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clusters_ = len(set(clean_data['cluster'])) - (1 if -1 in clean_data['cluster'] else 0)\n",
    "n_clusters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70414544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove repetition in labels by turning it into a set.\n",
    "unique_labels = set(clean_data['cluster'])\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d89667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "da331271",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '  (0, 6438)\\t0.5139505333593353\\n  (0, 10110)\\t0.8578198233077009\\n  (1, 13740)\\t0.30888789907267256\\n  (1, 5055)\\t0.18896026327243687\\n  (1, 12017)\\t0.17261293260929064\\n  (1, 10436)\\t0.28750796164580894\\n  (1, 7423)\\t0.16715295836858554\\n  (1, 1163)\\t0.20469115425715806\\n  (1, 5089)\\t0.12197329190550149\\n  (1, 2578)\\t0.24826142037841434\\n  (1, 14746)\\t0.25330227316562526\\n  (1, 12361)\\t0.17836085160496454\\n  (1, 6328)\\t0.13603205062744417\\n  (1, 6910)\\t0.11269328826657299\\n  (1, 2512)\\t0.15571295793821727\\n  (1, 17453)\\t0.19831384873016444\\n  (1, 9693)\\t0.32398597241178945\\n  (1, 6452)\\t0.1436511343103029\\n  (1, 12331)\\t0.19225201871003816\\n  (1, 14852)\\t0.24500844910124522\\n  (1, 9293)\\t0.2889572481635746\\n  (1, 4121)\\t0.17130241745695282\\n  (1, 15280)\\t0.20579546012830094\\n  (1, 6438)\\t0.2038069625544972\\n  (2, 7338)\\t0.27919037601370666\\n  :\\t:\\n  (39437, 17837)\\t0.16441238741089453\\n  (39437, 7639)\\t0.18739404042635016\\n  (39438, 284)\\t0.4592810458931541\\n  (39438, 10107)\\t0.6159070790675876\\n  (39438, 16496)\\t0.4500873191189535\\n  (39438, 6910)\\t0.45512723056969046\\n  (39439, 9030)\\t0.8523554268869602\\n  (39439, 6452)\\t0.5229629300976769\\n  (39440, 1561)\\t0.6394281208162028\\n  (39440, 2130)\\t0.6359217382675527\\n  (39440, 10963)\\t0.36160683826199613\\n  (39440, 12146)\\t0.2365918756643947\\n  (39441, 13947)\\t0.6789181101158692\\n  (39441, 4200)\\t0.40855632668619263\\n  (39441, 4644)\\t0.3500012814582205\\n  (39441, 15838)\\t0.2825255290924791\\n  (39441, 9081)\\t0.2410748262837331\\n  (39441, 9630)\\t0.22193131595554158\\n  (39441, 9426)\\t0.24991953745257572\\n  (39442, 9030)\\t0.8523554268869602\\n  (39442, 6452)\\t0.5229629300976769\\n  (39443, 3375)\\t0.7195154636877465\\n  (39443, 16022)\\t0.40979039984003424\\n  (39443, 9030)\\t0.46584494252651054\\n  (39443, 7639)\\t0.31202213901520554'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-8b1105a6eb20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m45\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mscatter_plot_points\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mC\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mordered\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse\u001b[0m \u001b[1;34m'np.ascontiguousarray'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \"\"\"\n\u001b[1;32m--> 376\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    395\u001b[0m                             'TruncatedSVD for a possible alternative.')\n\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m         X = self._validate_data(X, dtype=[np.float64, np.float32],\n\u001b[0m\u001b[0;32m    398\u001b[0m                                 ensure_2d=True, copy=self.copy)\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'no_validation'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    614\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '  (0, 6438)\\t0.5139505333593353\\n  (0, 10110)\\t0.8578198233077009\\n  (1, 13740)\\t0.30888789907267256\\n  (1, 5055)\\t0.18896026327243687\\n  (1, 12017)\\t0.17261293260929064\\n  (1, 10436)\\t0.28750796164580894\\n  (1, 7423)\\t0.16715295836858554\\n  (1, 1163)\\t0.20469115425715806\\n  (1, 5089)\\t0.12197329190550149\\n  (1, 2578)\\t0.24826142037841434\\n  (1, 14746)\\t0.25330227316562526\\n  (1, 12361)\\t0.17836085160496454\\n  (1, 6328)\\t0.13603205062744417\\n  (1, 6910)\\t0.11269328826657299\\n  (1, 2512)\\t0.15571295793821727\\n  (1, 17453)\\t0.19831384873016444\\n  (1, 9693)\\t0.32398597241178945\\n  (1, 6452)\\t0.1436511343103029\\n  (1, 12331)\\t0.19225201871003816\\n  (1, 14852)\\t0.24500844910124522\\n  (1, 9293)\\t0.2889572481635746\\n  (1, 4121)\\t0.17130241745695282\\n  (1, 15280)\\t0.20579546012830094\\n  (1, 6438)\\t0.2038069625544972\\n  (2, 7338)\\t0.27919037601370666\\n  :\\t:\\n  (39437, 17837)\\t0.16441238741089453\\n  (39437, 7639)\\t0.18739404042635016\\n  (39438, 284)\\t0.4592810458931541\\n  (39438, 10107)\\t0.6159070790675876\\n  (39438, 16496)\\t0.4500873191189535\\n  (39438, 6910)\\t0.45512723056969046\\n  (39439, 9030)\\t0.8523554268869602\\n  (39439, 6452)\\t0.5229629300976769\\n  (39440, 1561)\\t0.6394281208162028\\n  (39440, 2130)\\t0.6359217382675527\\n  (39440, 10963)\\t0.36160683826199613\\n  (39440, 12146)\\t0.2365918756643947\\n  (39441, 13947)\\t0.6789181101158692\\n  (39441, 4200)\\t0.40855632668619263\\n  (39441, 4644)\\t0.3500012814582205\\n  (39441, 15838)\\t0.2825255290924791\\n  (39441, 9081)\\t0.2410748262837331\\n  (39441, 9630)\\t0.22193131595554158\\n  (39441, 9426)\\t0.24991953745257572\\n  (39442, 9030)\\t0.8523554268869602\\n  (39442, 6452)\\t0.5229629300976769\\n  (39443, 3375)\\t0.7195154636877465\\n  (39443, 16022)\\t0.40979039984003424\\n  (39443, 9030)\\t0.46584494252651054\\n  (39443, 7639)\\t0.31202213901520554'"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, random_state = 45)\n",
    "\n",
    "scatter_plot_points = pca.fit_transform()\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f8b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot_points = pca.fit_transform(features.toarray())\n",
    "\n",
    "colors = ['green', 'blue', 'orange', 'red', 'black', 'violet', 'grey','yellow','green',\n",
    "          'blue', 'orange', 'red', 'black', 'violet', 'grey', 'yellow','black']\n",
    "\n",
    "x_axis = [o[0] for o in scatter_plot_points]\n",
    "                                        \n",
    "y_axis = [o[1] for o in scatter_plot_points]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(50, 50))\n",
    "\n",
    "ax.scatter(x_axis, y_axis, c=[colors[d] for d in model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cc3a65d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CubicalCover' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-fc5bb8e3bf52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfilter_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;31m#(columns=[0, 1])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Define cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcover\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCubicalCover\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_intervals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverlap_frac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# Choose clustering algorithm – default is DBSCAN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mclusterer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CubicalCover' is not defined"
     ]
    }
   ],
   "source": [
    "# Define filter function – can be any scikit-learn transformer\n",
    "filter_func = features.toarray#(columns=[0, 1])\n",
    "# Define cover\n",
    "cover = CubicalCover(n_intervals=10, overlap_frac=0.3)\n",
    "# Choose clustering algorithm – default is DBSCAN\n",
    "clusterer = model\n",
    "\n",
    "# Configure parallelism of clustering step\n",
    "n_jobs = 1\n",
    "\n",
    "# Initialise pipeline\n",
    "pipe = make_mapper_pipeline(\n",
    "    filter_func=filter_func,\n",
    "    cover=cover,\n",
    "    clusterer=clusterer,\n",
    "    verbose=False,\n",
    "    n_jobs=n_jobs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b62e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eaff709d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_static_mapper_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-a246ca2deffd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplot_static_mapper_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'scrollZoom'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_static_mapper_graph' is not defined"
     ]
    }
   ],
   "source": [
    "fig = plot_static_mapper_graph(pipe, data)\n",
    "fig.show(config={'scrollZoom': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "925e5626",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-ab9275521c2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create colors for the clusters.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSpectral\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Create colors for the clusters.\n",
    "colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9609894b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360769fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.plotting import plot_point_cloud\n",
    "\n",
    "# TDA magic\n",
    "from gtda.mapper import (\n",
    "    CubicalCover,\n",
    "    make_mapper_pipeline,\n",
    "    Projection,\n",
    "    plot_static_mapper_graph,\n",
    "    plot_interactive_mapper_graph,\n",
    "    MapperInteractivePlotter\n",
    ")\n",
    "\n",
    "# ML tools\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04240c84",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_point_cloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-dcf21c09dbb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_point_cloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_point_cloud' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "plot_point_cloud(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91bd8353",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FilterFunctionName' from 'gtda.mapper.filter' (C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gtda\\mapper\\filter.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-8ab98e4d9677>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgtda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFilterFunctionName\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'FilterFunctionName' from 'gtda.mapper.filter' (C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gtda\\mapper\\filter.py)"
     ]
    }
   ],
   "source": [
    "from gtda.mapper.filter import FilterFunctionName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13781dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(50,50))\n",
    "plt.scatter(features[0],features[1],c=clean_data['cluster'], cmap=matplotlib.colors.ListedColormap(colors),s=15)\n",
    "plt.title('DBSCAN Clustering',fontsize=20)\n",
    "plt.xlabel('Feature 1',fontsize=14)\n",
    "plt.ylabel('Feature 2',fontsize=14)\n",
    "plt.show()\n",
    "    \n",
    "    # Plot the datapoints that are clustered\n",
    "    #xy = model[class_member_mask & core_samples_mask]\n",
    "    #plt.scatter(xy[:, 0], xy[:, 1],s=50, c=[col], marker=u'o', alpha=0.5)\n",
    "\n",
    "    # Plot the outliers\n",
    "   #xy = model[class_member_mask & ~core_samples_mask]\n",
    "    #plt.scatter(xy[:, 0], xy[:, 1],s=50, c=[col], marker=u'o', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5597fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481d065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbca7356",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(model, aes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e671a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "clean_data[:,0]['cluster'] = model.labels_\n",
    "\n",
    "plt.figure(figsize=(50,50))\n",
    "plt.scatter(clean_data[0],clean_data[1],c=clean_data['cluster'] , cmap=matplotlib.colors.ListedColormap(colors),s=15)\n",
    "plt.title('DBSCAN Clustering',fontsize=20)\n",
    "plt.xlabel('Feature 1',fontsize=14)\n",
    "plt.ylabel('Feature 2',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e6b604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "words10 = nltk.word_tokenize(words9)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words11 = []\n",
    "for word in words10:\n",
    "    wor=lemmatizer.lemmatize(word)\n",
    "    words11.append(wor)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8de840",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = \" \".join(words11)\n",
    "\n",
    "cp1 =cp.split(' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8c9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents =words5.values.astype(\"U\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d08a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "documents =cp1.values.astype(\"U\")\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = ('english'))\n",
    "features = vectorizer.fit_transform(cp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb14cfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = features[:500]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc52096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show dimension 2 (0-indexed) of all 500 data points\n",
    "data[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9818027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise 3 of 64 dimensions\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(data[:,2], data[:,3], data[:,4], c='r', marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e8154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show one of the 8x8 images\n",
    "\n",
    "img = data[0].reshape(8,8)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e54847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0726bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa4942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show one of the 8x8 images\n",
    "import numpy\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb80fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tsne object which will reduce data to 2 dimensions\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f729b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2d = tsne.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa06c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot tSNE transformed data\n",
    "\n",
    "plt.scatter(data_2d[:,0], data_2d[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff29d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise again and highlight actual classes of data\n",
    "\n",
    "target_ids = range(len(digits.target_names))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colours = ['r', 'g', 'b', 'c', 'm', 'y', 'k', 'pink', 'orange', 'purple']\n",
    "\n",
    "for i, c, label in zip(target_ids, colours, digits.target_names):\n",
    "    plt.scatter(data_2d[labels == i, 0], data_2d[labels == i, 1], c=c, label=label)\n",
    "    pass\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff425791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8a85ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467358cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef14395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5e4c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5c2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagOfWordsA = x\n",
    "bagOfWordsB = y\n",
    "\n",
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsA:\n",
    "    numOfWordsA[word] += 1\n",
    "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsB:\n",
    "    numOfWordsB[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fadaff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "\n",
    "\n",
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e8522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06021c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaece964",
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = computeIDF([numOfWordsA, numOfWordsB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ed683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3110e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "df = pd.DataFrame([tfidfA, tfidfB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca0013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = \" \".join(x)\n",
    "c2 = \" \".join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bf4cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform([c1, c2])\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3e7a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb0e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc95465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise data\n",
    "x_axis = [o[0] for o in df]\n",
    "y_axis = [o[1] for o in df]\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "ax.scatter(x_axis, y_axis, c = 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dda090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tsne object which will reduce data to 2 dimensions\n",
    "tsne = TSNE(n_components=2, perplexity=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bb635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tSNE to data\n",
    "\n",
    "data_2d = tsne.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ddf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise data\n",
    "\n",
    "\n",
    "x_axis = [o[0] for o in data_2d]\n",
    "y_axis = [o[1] for o in data_2d]\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "ax.scatter(x_axis, y_axis, c = 'black')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef097bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a48b7bc",
   "metadata": {},
   "source": [
    "df.to_csv('tfidf.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bcb2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, random_state = 45)\n",
    "scatter_plot_points = pca.fit_transform(vectors.toarray(\n",
    "\n",
    "x_axis = [o[0] for o in scatter_plot_points]\n",
    "y_axis = [o[1] for o in scatter_plot_points]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(50, 50))\n",
    "\n",
    "ax.scatter(x_axis, y_axis, c=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83466f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "documents =bagofwords.values.astype(\"U\")\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = ('english'))\n",
    "features = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ecaf4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37383a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69830cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7138d86e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab015ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65e0c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf43fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a3eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and Lemmatizing\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "preprocessed_docs = []\n",
    "\n",
    "for doc in words6:\n",
    "    final_doc = []\n",
    "    for word in doc:\n",
    "        final_doc.append(porter.stem(word))\n",
    "        #final_doc.append(snowball.stem(word))\n",
    "        #final_doc.append(wordnet.lemmatize(word))\n",
    "    \n",
    "    preprocessed_docs.append(final_doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfdd25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2b0f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ad0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddd397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas \n",
    "\n",
    "corpus = words9\n",
    "# View most frequently occuring keywords\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
    "                   vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                       reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# Convert most freq words to dataframe for plotting bar plot, save as CSV\n",
    "top_words = get_top_n_words(corpus, n=20)\n",
    "top_df = pandas.DataFrame(top_words)\n",
    "top_df.columns=[\"Keyword\", \"Frequency\"]\n",
    "print(top_df)\n",
    "#top_df.to_csv(file_prefix + '_top_words.csv')\n",
    "\n",
    "# Barplot of most freq words\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "g = sns.barplot(x=\"Keyword\", y=\"Frequency\", data=top_df, palette=\"rainbow\")\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
    "#g.figure.savefig(file_prefix + \"_keyword.png\", bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a15ce9",
   "metadata": {},
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "data = clean_data['response'].apply(lambda x: len(str(x).split(\" \")))\n",
    "ds_count = len(data)\n",
    "\n",
    "for i in range(0, ds_count):\n",
    "    \n",
    "    text = re.sub('[^a-zA-Z]', ' ', str(clean_data['response'][i])\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    text = text.split()\n",
    "    lem  = WordNetLemmatizer()\n",
    "    text = [lem.lemmatize(word) for word in text if not word in stop_words] \n",
    "    text = \" \".join(text)\n",
    "    corpus.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d33cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequently occuring bigrams\n",
    "def get_top_n2_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(2,2),  \n",
    "            max_features=30000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# Convert most freq bigrams to dataframe for plotting bar plot, save as CSV\n",
    "top2_words = get_top_n2_words(corpus, n=20)\n",
    "top2_df = pandas.DataFrame(top2_words)\n",
    "top2_df.columns=[\"Bi-gram\", \"Frequency\"]\n",
    "print(top2_df)\n",
    "#top2_df.to_csv(file_prefix + '_bigrams.csv')\n",
    "\n",
    "# Barplot of most freq Bi-grams\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "h=sns.barplot(x=\"Bi-gram\", y=\"Frequency\", data=top2_df, palette=\"rainbow\")\n",
    "\n",
    "h.set_xticklabels(h.get_xticklabels(), rotation=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc7e097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequently occuring Tri-grams\n",
    "def get_top_n3_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(3,3), \n",
    "           max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# Convert most freq trigrams to dataframe for plotting bar plot, save as CSV\n",
    "top3_words = get_top_n3_words(corpus, n=20)\n",
    "top3_df = pandas.DataFrame(top3_words)\n",
    "top3_df.columns=[\"Tri-gram\", \"Frequency\"]\n",
    "print(top3_df)\n",
    "#top3_df.to_csv(file_prefix + '_trigrams.csv')\n",
    "\n",
    "# Barplot of most freq Tri-grams\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "j=sns.barplot(x=\"Tri-gram\", y=\"Frequency\", data=top3_df, palette=\"rainbow\")\n",
    "j.set_xticklabels(j.get_xticklabels(), rotation=75)\n",
    "#j.figure.savefig(file_prefix + \"_tri-gram.png\", bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73075881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary modules\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    " \n",
    "# Reads 'Youtube04-Eminem.csv' file\n",
    "df = raw_data\n",
    " \n",
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    " \n",
    "# iterate through the csv file\n",
    "for val in df.response:\n",
    "     \n",
    "    # typecaste each val to string\n",
    "    val = str(val)\n",
    " \n",
    "    # split the value\n",
    "    tokens = val.split()\n",
    "     \n",
    "    # Converts each token into lowercase\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "     \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    " \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8830f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(X)\n",
    "\n",
    "# Get feature names\n",
    "feature_names=cv.get_feature_names()\n",
    " \n",
    "# Fetch document for which keywords needs to be extracted\n",
    "doc=corpus[ds_count-1]\n",
    " \n",
    "# Generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dfdebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort tf_idf in descending order\n",
    "from scipy.sparse import coo_matrix\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=25):\n",
    "    \n",
    "    # Use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # Word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        # Keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    # Create tuples of feature,score\n",
    "    # Results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    return results\n",
    "\n",
    "# Sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "# Extract only the top n; n here is 25\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,25)\n",
    " \n",
    "# Print the results, save as CSV\n",
    "print(\"\\nAbstract:\")\n",
    "print(doc)\n",
    "print(\"\\nKeywords:\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf02de8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1748a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "text = comment_words\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "word_tokens = word_tokenize(text)\n",
    " \n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    " import yake\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "text =  filtered_sentence\n",
    "language = \"en\"\n",
    "max_ngram_size = 3\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 30\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(text)\n",
    "for kw in keywords:\n",
    "     print(kw)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "528f4c6f",
   "metadata": {},
   "source": [
    "from rake_nltk import Rake\n",
    "rake_nltk_var = Rake()\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 3\n",
    "deduplication_thresold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "numOfKeywords = 10\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(text)\n",
    "\n",
    "for kw in keywords:\n",
    "    print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c24c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "X_train = new_raw_data.values\n",
    "\n",
    "hc = AgglomerativeClustering(n_clusters=12, linkage='ward')\n",
    "hc = hc.fit(X_train)\n",
    "hc.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da15be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['Clusters'] = hc.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5875a2b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcedc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.to_csv('raw_hierar.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118a588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans = pd.read_csv('C:\\\\Users\\BRIGHT\\\\Desktop\\\\caproject.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
