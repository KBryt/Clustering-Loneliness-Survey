{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a655a527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Idme</th>\n",
       "      <th>Q9_Loneliness_Meaning_Qual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>203</td>\n",
       "      <td>-99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>237</td>\n",
       "      <td>Makes me feel empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>545</td>\n",
       "      <td>Not speaking to anyone for days. Being out wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>565</td>\n",
       "      <td>I am a very loving generous person but due to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>658</td>\n",
       "      <td>Unlikeable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Idme                         Q9_Loneliness_Meaning_Qual\n",
       "0   203                                                -99\n",
       "1   237                                Makes me feel empty\n",
       "2   545  Not speaking to anyone for days. Being out wit...\n",
       "3   565  I am a very loving generous person but due to ...\n",
       "4   658                                         Unlikeable"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "## Imports (code & data)\n",
    "\n",
    "import pandas as pd\n",
    "import yake\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "raw_data = pd.read_csv('C:\\\\Users\\BRIGHT\\\\Desktop\\\\caproject.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d636d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming column\n",
    "\n",
    "raw_data = raw_data.rename(columns = {'Q9_Loneliness_Meaning_Qual': 'response'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48000f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = raw_data[raw_data['response'] != '-99']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c94ceb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "clean_data1 = clean_data.response.str.cat(sep=' ')\n",
    "\n",
    "words0 = word_tokenize(clean_data1)\n",
    "\n",
    "\n",
    "words1 = [word for word in words0 if len(word) > 2]\n",
    "\n",
    "\n",
    "#Remove numbers\n",
    "words2 = [word for word in words1 if not word.isnumeric()]\n",
    "\n",
    "#Remove non breaking space\n",
    "words3 = [word for word in words2 if word != \"nbsp\"]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "words4 = [w for w in words3 if not w in stop_words]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#lowercase all words\n",
    "\n",
    "words5 = [word.lower() for word in words4]\n",
    "\n",
    "stop_words = [\"loneliness\",\"or\",\"n't\"]\n",
    "\n",
    "tokens = words5\n",
    "\n",
    "clean_data3 = tokens[:]\n",
    "\n",
    "for token in tokens:\n",
    "    if token in stop_words:\n",
    "        clean_data3.remove(token)\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "words6 = ' '.join(ch for ch in clean_data3 if ch not in exclude)\n",
    "\n",
    "for char in string.punctuation:\n",
    "    words7 = words6.replace(char, ' ')\n",
    "\n",
    "words8 = words7.replace('.', ' ')\n",
    "words9 = words8.replace('/', ' ')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a7af0690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "words10 = nltk.word_tokenize(words9)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words11 = []\n",
    "for word in words10:\n",
    "    wor=lemmatizer.lemmatize(word)\n",
    "    words11.append(wor)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "24d683c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = \" \".join(words11)\n",
    "\n",
    "cp1 =cp.split(' ')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "y, x = train_test_split(cp1, test_size=0.50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d314bfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "uniqueWords = set(x).union(set(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c55e1c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagOfWordsA = x\n",
    "bagOfWordsB = y\n",
    "\n",
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsA:\n",
    "    numOfWordsA[word] += 1\n",
    "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsB:\n",
    "    numOfWordsB[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3b865c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "\n",
    "\n",
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6a5edcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7c1f600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b8688619",
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = computeIDF([numOfWordsA, numOfWordsB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "902fb2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a5456fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "df = pd.DataFrame([tfidfA, tfidfB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "20d7361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = \" \".join(x)\n",
    "c2 = \" \".join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c2a53795",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform([c1, c2])\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "97e9c5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>000s</th>\n",
       "      <th>00am</th>\n",
       "      <th>0ne</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>10th</th>\n",
       "      <th>10yr</th>\n",
       "      <th>10yrs</th>\n",
       "      <th>...</th>\n",
       "      <th>والاشياء</th>\n",
       "      <th>والسفر</th>\n",
       "      <th>والفقر</th>\n",
       "      <th>وبين</th>\n",
       "      <th>وتكرر</th>\n",
       "      <th>وزوجية</th>\n",
       "      <th>وعدم</th>\n",
       "      <th>ولاكن</th>\n",
       "      <th>ولايسمح</th>\n",
       "      <th>يلائمها</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 15436 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        000      000s      00am       0ne        10       100      1000  \\\n",
       "0  0.000193  0.000068  0.000068  0.000068  0.000144  0.000048  0.000048   \n",
       "1  0.000048  0.000000  0.000000  0.000000  0.000144  0.000048  0.000048   \n",
       "\n",
       "       10th      10yr     10yrs  ...  والاشياء    والسفر    والفقر      وبين  \\\n",
       "0  0.000048  0.000068  0.000135  ...  0.000068  0.000000  0.000000  0.000000   \n",
       "1  0.000048  0.000000  0.000000  ...  0.000000  0.000067  0.000067  0.000067   \n",
       "\n",
       "      وتكرر    وزوجية      وعدم     ولاكن   ولايسمح   يلائمها  \n",
       "0  0.000000  0.000068  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.000067  0.000000  0.000067  0.000067  0.000067  0.000067  \n",
       "\n",
       "[2 rows x 15436 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2a6b6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7704cc61",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-e137a2ffd824>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdenselist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenselist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "plt.scatter(denselist[:,0], denselist[:,1])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f0389df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3b379b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "134b07de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tfidf.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2cd18b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:461: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ = (S ** 2) / (n_samples - 1)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-360ce30625bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mx_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscatter_plot_points\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0my_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscatter_plot_points\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-104-360ce30625bc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mx_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscatter_plot_points\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0my_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscatter_plot_points\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=1, random_state = 45)\n",
    "scatter_plot_points = pca.fit_transform(vectors.toarray())\n",
    "\n",
    "\n",
    "x_axis = [o[0] for o in scatter_plot_points]\n",
    "y_axis = [o[1] for o in scatter_plot_points]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(50, 50))\n",
    "\n",
    "ax.scatter(x_axis, y_axis, c=[colors[d] for d in kmeans_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c8a7808b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-01613277ef47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mbagofwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"U\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "documents =bagofwords.values.astype(\"U\")\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = ('english'))\n",
    "features = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d22856f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bf0df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c8deb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0770f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca1e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6348f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900402c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a38a3eca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words6' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ebfccfe2478a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mpreprocessed_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mfinal_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'words6' is not defined"
     ]
    }
   ],
   "source": [
    "# Stemming and Lemmatizing\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "preprocessed_docs = []\n",
    "\n",
    "for doc in words6:\n",
    "    final_doc = []\n",
    "    for word in doc:\n",
    "        final_doc.append(porter.stem(word))\n",
    "        #final_doc.append(snowball.stem(word))\n",
    "        #final_doc.append(wordnet.lemmatize(word))\n",
    "    \n",
    "    preprocessed_docs.append(final_doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfdd25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2b0f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ad0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddd397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas \n",
    "\n",
    "corpus = words9\n",
    "# View most frequently occuring keywords\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
    "                   vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                       reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# Convert most freq words to dataframe for plotting bar plot, save as CSV\n",
    "top_words = get_top_n_words(corpus, n=20)\n",
    "top_df = pandas.DataFrame(top_words)\n",
    "top_df.columns=[\"Keyword\", \"Frequency\"]\n",
    "print(top_df)\n",
    "#top_df.to_csv(file_prefix + '_top_words.csv')\n",
    "\n",
    "# Barplot of most freq words\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "g = sns.barplot(x=\"Keyword\", y=\"Frequency\", data=top_df, palette=\"rainbow\")\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
    "#g.figure.savefig(file_prefix + \"_keyword.png\", bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a15ce9",
   "metadata": {},
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "data = clean_data['response'].apply(lambda x: len(str(x).split(\" \")))\n",
    "ds_count = len(data)\n",
    "\n",
    "for i in range(0, ds_count):\n",
    "    \n",
    "    text = re.sub('[^a-zA-Z]', ' ', str(clean_data['response'][i])\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    text = text.split()\n",
    "    lem  = WordNetLemmatizer()\n",
    "    text = [lem.lemmatize(word) for word in text if not word in stop_words] \n",
    "    text = \" \".join(text)\n",
    "    corpus.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d33cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequently occuring bigrams\n",
    "def get_top_n2_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(2,2),  \n",
    "            max_features=30000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# Convert most freq bigrams to dataframe for plotting bar plot, save as CSV\n",
    "top2_words = get_top_n2_words(corpus, n=20)\n",
    "top2_df = pandas.DataFrame(top2_words)\n",
    "top2_df.columns=[\"Bi-gram\", \"Frequency\"]\n",
    "print(top2_df)\n",
    "#top2_df.to_csv(file_prefix + '_bigrams.csv')\n",
    "\n",
    "# Barplot of most freq Bi-grams\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "h=sns.barplot(x=\"Bi-gram\", y=\"Frequency\", data=top2_df, palette=\"rainbow\")\n",
    "\n",
    "h.set_xticklabels(h.get_xticklabels(), rotation=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc7e097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequently occuring Tri-grams\n",
    "def get_top_n3_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(3,3), \n",
    "           max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# Convert most freq trigrams to dataframe for plotting bar plot, save as CSV\n",
    "top3_words = get_top_n3_words(corpus, n=20)\n",
    "top3_df = pandas.DataFrame(top3_words)\n",
    "top3_df.columns=[\"Tri-gram\", \"Frequency\"]\n",
    "print(top3_df)\n",
    "#top3_df.to_csv(file_prefix + '_trigrams.csv')\n",
    "\n",
    "# Barplot of most freq Tri-grams\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "j=sns.barplot(x=\"Tri-gram\", y=\"Frequency\", data=top3_df, palette=\"rainbow\")\n",
    "j.set_xticklabels(j.get_xticklabels(), rotation=75)\n",
    "#j.figure.savefig(file_prefix + \"_tri-gram.png\", bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73075881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary modules\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    " \n",
    "# Reads 'Youtube04-Eminem.csv' file\n",
    "df = raw_data\n",
    " \n",
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    " \n",
    "# iterate through the csv file\n",
    "for val in df.response:\n",
    "     \n",
    "    # typecaste each val to string\n",
    "    val = str(val)\n",
    " \n",
    "    # split the value\n",
    "    tokens = val.split()\n",
    "     \n",
    "    # Converts each token into lowercase\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "     \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    " \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8830f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(X)\n",
    "\n",
    "# Get feature names\n",
    "feature_names=cv.get_feature_names()\n",
    " \n",
    "# Fetch document for which keywords needs to be extracted\n",
    "doc=corpus[ds_count-1]\n",
    " \n",
    "# Generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dfdebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort tf_idf in descending order\n",
    "from scipy.sparse import coo_matrix\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=25):\n",
    "    \n",
    "    # Use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # Word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        # Keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    # Create tuples of feature,score\n",
    "    # Results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    return results\n",
    "\n",
    "# Sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "# Extract only the top n; n here is 25\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,25)\n",
    " \n",
    "# Print the results, save as CSV\n",
    "print(\"\\nAbstract:\")\n",
    "print(doc)\n",
    "print(\"\\nKeywords:\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf02de8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1748a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "text = comment_words\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "word_tokens = word_tokenize(text)\n",
    " \n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    " import yake\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "text =  filtered_sentence\n",
    "language = \"en\"\n",
    "max_ngram_size = 3\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 30\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(text)\n",
    "for kw in keywords:\n",
    "     print(kw)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "528f4c6f",
   "metadata": {},
   "source": [
    "from rake_nltk import Rake\n",
    "rake_nltk_var = Rake()\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 3\n",
    "deduplication_thresold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "numOfKeywords = 10\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(text)\n",
    "\n",
    "for kw in keywords:\n",
    "    print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c24c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "X_train = new_raw_data.values\n",
    "\n",
    "hc = AgglomerativeClustering(n_clusters=12, linkage='ward')\n",
    "hc = hc.fit(X_train)\n",
    "hc.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da15be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['Clusters'] = hc.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5875a2b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcedc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.to_csv('raw_hierar.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118a588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans = pd.read_csv('C:\\\\Users\\BRIGHT\\\\Desktop\\\\caproject.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
