{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a655a527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Idme</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>Q9_Loneliness_Meaning_Qual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>203</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>237</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Makes me feel empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>545</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Not speaking to anyone for days. Being out wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>565</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>I am a very loving generous person but due to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>658</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Unlikeable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Idme  gender   age                         Q9_Loneliness_Meaning_Qual\n",
       "0   203     2.0  16.0                                                -99\n",
       "1   237     2.0  16.0                                Makes me feel empty\n",
       "2   545     2.0  16.0  Not speaking to anyone for days. Being out wit...\n",
       "3   565     1.0  16.0  I am a very loving generous person but due to ...\n",
       "4   658     2.0  16.0                                         Unlikeable"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "## Imports (code & data)\n",
    "\n",
    "import pandas as pd\n",
    "import yake\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "raw_data = pd.read_csv(r'C:\\Users\\BRIGHT\\raw.csv')\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d636d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming column\n",
    "\n",
    "raw_data = raw_data.rename(columns = {'Q9_Loneliness_Meaning_Qual': 'response'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48000f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = raw_data[raw_data['response'] != '-99']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b4baa7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39444, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aba738de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-1c5aab41cc2b>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['response'] = clean_data['response'].str.lower()\n",
      "<ipython-input-5-1c5aab41cc2b>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['response'] = clean_data['response'].str.replace('loneliness', '')\n",
      "<ipython-input-5-1c5aab41cc2b>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['response'] = clean_data['response'].str.replace('loneiness', '')\n",
      "<ipython-input-5-1c5aab41cc2b>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['response'] = clean_data['response'].str.replace('loneness', '')\n",
      "<ipython-input-5-1c5aab41cc2b>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['response'] = clean_data['response'].str.replace('aloneliness', '')\n",
      "<ipython-input-5-1c5aab41cc2b>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['response'] = clean_data['response'].str.replace('loneness', '')\n",
      "<ipython-input-5-1c5aab41cc2b>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['response'] = clean_data['response'].str.replace('/', ' ')\n",
      "<ipython-input-5-1c5aab41cc2b>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['response'] = clean_data['response'].str.replace('\\n', ' ')\n",
      "<ipython-input-5-1c5aab41cc2b>:9: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  clean_data['response'] = clean_data['response'].str.replace('.', '')\n",
      "<ipython-input-5-1c5aab41cc2b>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['response'] = clean_data['response'].str.replace('.', '')\n",
      "<ipython-input-5-1c5aab41cc2b>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_data['response'] = clean_data['response'].str.replace(',', '')\n"
     ]
    }
   ],
   "source": [
    "clean_data['response'] = clean_data['response'].str.lower()\n",
    "clean_data['response'] = clean_data['response'].str.replace('loneliness', '')\n",
    "clean_data['response'] = clean_data['response'].str.replace('loneiness', '')\n",
    "clean_data['response'] = clean_data['response'].str.replace('loneness', '')\n",
    "clean_data['response'] = clean_data['response'].str.replace('aloneliness', '')\n",
    "clean_data['response'] = clean_data['response'].str.replace('loneness', '')\n",
    "clean_data['response'] = clean_data['response'].str.replace('/', ' ')\n",
    "clean_data['response'] = clean_data['response'].str.replace('\\n', ' ')\n",
    "clean_data['response'] = clean_data['response'].str.replace('.', '')\n",
    "clean_data['response'] = clean_data['response'].str.replace(',', '')\n",
    "                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4a89e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "words10 = nltk.word_tokenize(clean_data['response'].str.cat(sep=' '))\n",
    "  \n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "words13 = [w for w in words10 if not w in stop_words]\n",
    "#lowercase all words\n",
    "\n",
    "words14 = [word.lower() for word in words13]\n",
    "\n",
    "stop_words = [\"loneliness\",\"or\",\"n't\"]\n",
    "\n",
    "tokens = words14\n",
    "\n",
    "clean_data3 = tokens[:]\n",
    "\n",
    "for token in tokens:\n",
    "    if token in stop_words:\n",
    "        clean_data3.remove(token)\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "words6 = ' '.join(ch for ch in clean_data3 if ch not in exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a84d8491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words11 = []\n",
    "for word in words6:\n",
    "    wor=lemmatizer.lemmatize(word)\n",
    "    words11.append(wor)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47c15e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 1\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 20\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(words11)\n",
    "for kw in keywords:\n",
    "     new = kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20f50a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(words6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd5a60c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lonelyness pure', 0.6053),\n",
       " ('lonelyness feel', 0.6019),\n",
       " ('lonely emptiness', 0.5981),\n",
       " ('lonely functioning', 0.5934),\n",
       " ('feel lonelynot', 0.5918)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_model.extract_keywords(words6, keyphrase_ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf0940ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model.extract_keywords(clean_data['response'], highlight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "078c2943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Idme</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>237</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>makes me feel empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>545</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>not speaking to anyone for days being out with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>565</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>i am a very loving generous person but due to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>658</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>unlikeable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1044</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>feeling like you aren't connected to the world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51227</th>\n",
       "      <td>51256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>feeling isolated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51234</th>\n",
       "      <td>51263</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>need to avoid boring people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51242</th>\n",
       "      <td>51271</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>isolation  lack of support   review for diffic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51244</th>\n",
       "      <td>51273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>feeling isolated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51246</th>\n",
       "      <td>51275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>being isolated having no one to talk to or con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39444 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Idme  gender   age                                           response\n",
       "1        237     2.0  16.0                                makes me feel empty\n",
       "2        545     2.0  16.0  not speaking to anyone for days being out with...\n",
       "3        565     1.0  16.0  i am a very loving generous person but due to ...\n",
       "4        658     2.0  16.0                                         unlikeable\n",
       "5       1044     1.0  16.0  feeling like you aren't connected to the world...\n",
       "...      ...     ...   ...                                                ...\n",
       "51227  51256     NaN   NaN                                   feeling isolated\n",
       "51234  51263     NaN   NaN                        need to avoid boring people\n",
       "51242  51271     NaN   NaN  isolation  lack of support   review for diffic...\n",
       "51244  51273     NaN   NaN                                   feeling isolated\n",
       "51246  51275     NaN   NaN  being isolated having no one to talk to or con...\n",
       "\n",
       "[39444 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6de5babf",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 466. GiB for an array with shape (353529, 353529) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6935a9a5fc6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m kw_model.extract_keywords(words6, keyphrase_ngram_range=(3, 3), stop_words='english', \n\u001b[0m\u001b[0;32m      2\u001b[0m                               use_maxsum=True, nr_candidates=20, top_n=10)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keybert\\_model.py\u001b[0m in \u001b[0;36mextract_keywords\u001b[1;34m(self, docs, candidates, keyphrase_ngram_range, stop_words, top_n, min_df, use_maxsum, use_mmr, diversity, nr_candidates, vectorizer, highlight, seed_keywords)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             keywords = self._extract_keywords_single_doc(doc=docs,\n\u001b[0m\u001b[0;32m    114\u001b[0m                                                          \u001b[0mcandidates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                                                          \u001b[0mkeyphrase_ngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeyphrase_ngram_range\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keybert\\_model.py\u001b[0m in \u001b[0;36m_extract_keywords_single_doc\u001b[1;34m(self, doc, candidates, keyphrase_ngram_range, stop_words, top_n, use_maxsum, use_mmr, diversity, nr_candidates, vectorizer, seed_keywords)\u001b[0m\n\u001b[0;32m    192\u001b[0m                 \u001b[0mkeywords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmmr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0muse_maxsum\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m                 \u001b[0mkeywords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_sum_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnr_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keybert\\_maxsum.py\u001b[0m in \u001b[0;36mmax_sum_similarity\u001b[1;34m(doc_embedding, word_embeddings, words, top_n, nr_candidates)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# Calculate distances and extract keywords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mdistances_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m# Get 2*top_n words as candidates based on cosine similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[0mY_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m     K = safe_sparse_dot(X_normalized, Y_normalized.T,\n\u001b[0m\u001b[0;32m   1189\u001b[0m                         dense_output=dense_output)\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 466. GiB for an array with shape (353529, 353529) and data type float32"
     ]
    }
   ],
   "source": [
    "kw_model.extract_keywords(words6, keyphrase_ngram_range=(3, 3), stop_words='english', \n",
    "                              use_maxsum=True, nr_candidates=20, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a815a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model.extract_keywords(clean_data['response'], highlight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "features = ['age_group', 'gender_cat']\n",
    "\n",
    "for f in features:\n",
    "    sns.countplot(x = f, data = clean_data, palette = 'Set3')# hue = 'Good Loan')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dccbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\n",
    "documents1 = clean_data['response'].values.astype(\"U\")\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = ('english'))\n",
    "features = vectorizer.fit_transform(documents1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = KMeans(n_clusters=5, init='k-means++', max_iter=300, n_init=3, tol=0.0001)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# \"predictions\" for new data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8d0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_indices = model.fit_predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de951bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(kmeans_indices, return_counts=True)\n",
    "counts = counts.reshape(1,5)\n",
    "\n",
    "# calculating the Counts of the cluster\n",
    "\n",
    "\n",
    "# Creating a datagrame\n",
    "countscldf = pd.DataFrame(counts, columns = [\"Cluster 0\",\"Cluster 1\",\"Cluster 2\", \"Cluster 3\",\"Cluster 4\"])\n",
    "\n",
    "# display\n",
    "countscldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1488fb7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for j in order_centroids[i, :50]: #print out 10 feature terms of each cluster\n",
    "        print (' %s' % terms[j])\n",
    "    print('------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff490717",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['cluster'] = model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3814007",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8018955",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.to_csv('age_cluster.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e96c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['green', 'blue', 'orange', 'red', 'black']\n",
    "\n",
    "x_axis = [o[0] for o in scatter_plot_points]\n",
    "y_axis = [o[1] for o in scatter_plot_points]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(50, 50))\n",
    "\n",
    "ax.scatter(x_axis, y_axis, c=[colors[d] for d in kmeans_indices])\n",
    "\n",
    "#for i, txt in enumerate(reference_text['response']):\n",
    "    #ax.annotate(txt[0:5], (x_axis[i], y_axis[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51184f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "#from scipy.sparse import csr_matrix\n",
    "#import numpy as np\n",
    "#np.random.seed(0)\n",
    "#\n",
    "svd = TruncatedSVD(n_components=2, n_iter=7, random_state=42)\n",
    "T_SVD = svd.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f43f622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8378a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36d7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "words10 = nltk.word_tokenize(clean_data['response'].str.cat(sep=' '))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words11 = []\n",
    "for word in words10:\n",
    "    wor=lemmatizer.lemmatize(word)\n",
    "    words11.append(wor)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b9c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 4\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 10\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(words6)\n",
    "for kw in keywords:\n",
    "     print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d36cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words12 = []\n",
    "for word in words11:\n",
    "    wor=lemmatizer.lemmatize(word)\n",
    "    words12.append(wor)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a402661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "words13 = [w for w in words12 if not w in stop_words]\n",
    "#lowercase all words\n",
    "\n",
    "words14 = [word.lower() for word in words13]\n",
    "\n",
    "stop_words = [\"loneliness\",\"or\",\"n't\"]\n",
    "\n",
    "tokens = words14\n",
    "\n",
    "clean_data3 = tokens[:]\n",
    "\n",
    "for token in tokens:\n",
    "    if token in stop_words:\n",
    "        clean_data3.remove(token)\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "words6 = ' '.join(ch for ch in clean_data3 if ch not in exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ab19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 1\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 50\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(words6)\n",
    "for kw in keywords:\n",
    "     print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726ad6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "words10 = nltk.word_tokenize(words6)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words11 = []\n",
    "for word in words10:\n",
    "    wor=lemmatizer.lemmatize(word)\n",
    "    words11.append(wor)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e0a17d",
   "metadata": {},
   "outputs": [],
   "source": [
    " import yake\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 1\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 50\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(words6)\n",
    "for kw in keywords:\n",
    "     print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc491ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    " import yake\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 4\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 50\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(words6)\n",
    "for kw in keywords:\n",
    "     print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7240ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 5\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 50\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(words6)\n",
    "for kw in keywords:\n",
    "     print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(words6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b6f783",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model.extract_keywords(words6, keyphrase_ngram_range=(3, 3), stop_words='english', \n",
    "                              use_maxsum=True, nr_candidates=20, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model.extract_keywords(words6, keyphrase_ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c418f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model.extract_keywords(words6, keyphrase_ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca9a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1ea27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model.extract_keywords(words6, keyphrase_ngram_range=(3, 3), stop_words='english', \n",
    "                              use_maxsum=True, nr_candidates=20, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0853c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kw_model.extract_keywords(clean_data['response'], highlight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a585c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef66e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f926cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "900afcf5",
   "metadata": {},
   "source": [
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=30, max_words=30000, background_color=\"white\").generate(words6)\n",
    "plt.figure(figsize = (15, 15), facecolor = None)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "wordcloud.to_file('loneliness_wordcloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a6038",
   "metadata": {},
   "source": [
    " import yake\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 5\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 10\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(words6)\n",
    "for kw in keywords:\n",
    "     print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758f5cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 1\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 5\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(words6)\n",
    "for kw in keywords:\n",
    "     print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a5367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "documents =clean_data['response'].values.astype(\"U\")\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = ('english'))\n",
    "features = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2635c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737a3087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "#from scipy.sparse import csr_matrix\n",
    "#import numpy as np\n",
    "#np.random.seed(0)\n",
    "#\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "T_SVD = svd.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c90cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_SVD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d34889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_axis = [o[0] for o in T_SVD]\n",
    "y_axis = [o[1] for o in T_SVD]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(50, 50))\n",
    "\n",
    "ax.scatter(x_axis, y_axis, color = 'black', alpha = 0.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f8801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.manifold import TSNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0410fd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, perplexity=20)\n",
    "data_2d = tsne.fit_transform(T_SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db5f5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e54808",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= DBSCAN(eps = 0.7, min_samples = 25).fit(data_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da227077",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd8524",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_samples_mask = np.zeros_like(cluster, dtype=bool)\n",
    "core_samples_mask[model.core_sample_indices_] = True\n",
    "core_samples_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731fc79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters_ = len(set(cluster)) - (1 if -1 in cluster else 0)\n",
    "n_noise_ = list(cluster).count(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57689271",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292abe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "data_2d_cluster = (data_2d[\"cluster\"] >= 0)\n",
    "\n",
    "silhouette_score(data_2d_cluster[[x_axis, y_axis]], data_2d_cluster[(cluster)] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d26952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = [o[0] for o in data_2d]\n",
    "y_axis = [o[1] for o in data_2d]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f9847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_noise_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1da379",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_axis = [o[0] for o in data_2d]\n",
    "y_axis = [o[1] for o in data_2d]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(50, 50))\n",
    "\n",
    "ax.scatter(x_axis, y_axis, color = 'black', alpha = 0.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0bd186",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set(cluster)\n",
    "plt.figure(figsize=(30,30))\n",
    "\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    respondent_mask = cluster == k\n",
    "    \n",
    "    xy = data_2d[respondent_mask & core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=14,\n",
    "    )\n",
    "\n",
    "    xy = data_2d[respondent_mask & ~core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=6,\n",
    "    )\n",
    "\n",
    "plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e646df5e",
   "metadata": {},
   "source": [
    "from ggplot import *\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from pandas import Timestamp\n",
    "\n",
    "labslist = [\"Noise\"]\n",
    "labslist = labslist + [\"Cluster\" + str(i) for i in range(1, len(set(cluster)))]\n",
    "\n",
    "cluster = model.labels_\n",
    "\n",
    "x_axis = [o[0] for o in data_2d]\n",
    "y_axis = [o[1] for o in data_2d]\n",
    "\n",
    "\n",
    "ggplot(data_2d, aes(x_axis, y_axis, color = \"factor(cluster)\")) + geom_point() + theme_minimal()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7fe893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the points with colors\n",
    "plt.figure(figsize=(30,30))\n",
    "\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = 'k'\n",
    "\n",
    "    class_member_mask = (cluster == k)\n",
    "   \n",
    "    # Plot the datapoints that are clustered\n",
    "    xy = data_2d[class_member_mask & core_samples_mask]\n",
    "    plt.scatter(xy[:, 0], xy[:, 1],s=50, c=[col], marker=u'o', alpha=0.2, )\n",
    "\n",
    "    # Plot the outliers\n",
    "    xy= data_2d[class_member_mask & ~core_samples_mask]\n",
    "    plt.scatter(xy[:, 0], xy[:, 1],s=50, c=[col], marker=u'o', alpha=0.2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c7a9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "# Data viz\n",
    "from gtda.plotting import plot_point_cloud\n",
    "\n",
    "# TDA magic\n",
    "from gtda.mapper import (\n",
    "    CubicalCover,\n",
    "    make_mapper_pipeline,\n",
    "    Projection,\n",
    "    plot_static_mapper_graph,\n",
    "    plot_interactive_mapper_graph,\n",
    "    MapperInteractivePlotter\n",
    ")\n",
    "\n",
    "\n",
    "# Define cover\n",
    "cover = CubicalCover(n_intervals=10, overlap_frac=0.5)\n",
    "# Choose clustering algorithm – default is DBSCAN\n",
    "\n",
    "# Configure parallelism of clustering step\n",
    "n_jobs = 1\n",
    "\n",
    "# Initialise pipeline\n",
    "pipe = make_mapper_pipeline(\n",
    "    cover=cover,\n",
    "    clusterer=model,\n",
    "    verbose=False,\n",
    "    n_jobs=n_jobs,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eff93e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_static_mapper_graph(pipe, data_2d)\n",
    "fig.show(config={'scrollZoom': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a6fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "data_2d_cluster = (data_2d[\"cluster\"] >= 0)\n",
    "\n",
    "silhouette_score(data_2d_cluster[[x_axis, y_axis]], data_2d_cluster[(cluster)] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b44d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1= DBSCAN(eps = 0.6, min_samples = 25)\n",
    "cover = CubicalCover(n_intervals=10, overlap_frac=0.3)\n",
    "# Choose clustering algorithm – default is DBSCAN\n",
    "\n",
    "# Configure parallelism of clustering step\n",
    "n_jobs = 1\n",
    "\n",
    "# Initialise pipeline\n",
    "pipe = make_mapper_pipeline(\n",
    "    cover=cover,\n",
    "    clusterer=model1,\n",
    "    verbose=False,\n",
    "    n_jobs=n_jobs,\n",
    ")\n",
    "\n",
    "fig = plot_static_mapper_graph(pipe, data_2d)\n",
    "fig.show(config={'scrollZoom': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56785c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_interactive_mapper_graph(pipe, data_2d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6185bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the plotter object\n",
    "MIP = MapperInteractivePlotter(pipe, data_2d)\n",
    "\n",
    "# Generate interactive widget\n",
    "MIP.plot(color_data=data_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604831c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Attributes created by `.plot` and updated during the interactive session:\\n\",\n",
    "      [attr for attr in dir(MIP) if attr.endswith(\"_\") and attr[0] != \"_\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008f2372",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = pipe.fit_transform(data_2d)\n",
    "node_elements = graph.vs[\"node_elements\"]\n",
    "print(f\"There are {len(node_elements)} nodes.\\nThe first node consists of row indices {node_elements[0]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aea2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_static_mapper_graph(\n",
    "    pipe, data_2d, node_color_statistic=np.arange(len(node_elements))\n",
    ")\n",
    "fig.show(config={'scrollZoom': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cb2e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_2d, columns=[\"x\", \"y\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f95909",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_params(filter_func=Projection(columns=[\"x\", \"y\"]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e5648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_static_mapper_graph(pipe, df, color_data=df)\n",
    "fig.show(config={'scrollZoom': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba06407",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Circle\"] = df[\"x\"] ** 2 + df[\"y\"] ** 2 < 0.25\n",
    "df[\"Circle\"] = df[\"Circle\"].replace([False, True], [\"A\", \"B\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640326c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_data = pd.get_dummies(df[\"Circle\"], prefix=\"Circle\")\n",
    "\n",
    "fig = plot_static_mapper_graph(pipe, df[[\"x\", \"y\"]], color_data=color_data)\n",
    "fig.show(config={'scrollZoom': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9cb6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = pipe.fit_transform(df[[\"x\", \"y\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374f3c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.vs.attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21270214",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_id = 0\n",
    "node_elements = graph.vs[\"node_elements\"]\n",
    "\n",
    "print(f\"\"\"\n",
    "Node ID: {node_id}\n",
    "Node elements: {node_elements[node_id]}\n",
    "Data points: {data_2d[node_elements[node_id]]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d8a796",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_func = np.sum\n",
    "\n",
    "pipe = make_mapper_pipeline(\n",
    "    filter_func=filter_func,\n",
    "    cover=cover,\n",
    "    clusterer=model,\n",
    "    verbose=True,\n",
    "    n_jobs=n_jobs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22aaea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_static_mapper_graph(pipe, data_2d)\n",
    "fig.show(config={'scrollZoom': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a6018c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a087b2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Newcluster = model.labels_\n",
    "Newcluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131b775c",
   "metadata": {},
   "source": [
    "k = 5\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for j in order_centroids[i, :10]: #print out 10 feature terms of each cluster\n",
    "        print (' %s' % terms[j])\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da15be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['Clusters'] = hc.labels_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
